{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\special\\__init__.py:601: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._ufuncs import *\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\linalg\\basic.py:20: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._solve_toeplitz import levinson\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\linalg\\__init__.py:188: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._decomp_update import *\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\special\\_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\lil.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import _csparsetools\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:152: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\csgraph\\_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:154: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:156: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:157: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from . import vonmises_cython\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\stats\\stats.py:187: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from ._rank import rankdata, tiecorrect\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\io\\matlab\\mio4.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .mio_utils import squeeze_element, chars_to_strings\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\io\\matlab\\mio5.py:98: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .mio5_utils import VarReader5\n",
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\spatial\\__init__.py:91: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .ckdtree import *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CHATHC01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHATHC01\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\spatial\\__init__.py:92: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  from .qhull import *\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named textblob",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fb119af52b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPORTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCOSINE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMEANS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHIERARCHICAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named textblob"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import praw\n",
    "import datetime\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from pattern.vector import Document, Model, TFIDF, TF, LEMMA, PORTER, COSINE, KMEANS, HIERARCHICAL\n",
    "\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "def tokenize_comment(comment): #converts comment into non-stopword, non-punctuation space-delimited string\n",
    "    tokens=nltk.wordpunct_tokenize(comment) #makes individual words from comments\n",
    "    tokens=[token.lower() for token in tokens if token.lower() not in english_stops] #removes stopwords\n",
    "    tokens = [token for token in tokens if token not in string.punctuation] #removes punctuation\n",
    "    #print sorted(tokens)[0:20] #check to see if tokens are meaningful\n",
    "    tokenstring='' #initialize empty string\n",
    "    for x in tokens:\n",
    "        tokenstring=tokenstring+x+' ' #make big space-limited token string\n",
    "    return tokenstring\n",
    "\n",
    "\n",
    "def collect_comments(subreddit_string, count, startdate, enddate, freqspec):\n",
    "\n",
    "    #usage: collect_comments('opiates',None,'11/14/2010','11/16/2015','D')\n",
    "    #subreddit_string is name of subreddit, e.g. 'opiates'\n",
    "    #count is number of submissions to retrieve per frequency interval (as defined by freqspec). Can be set to None, which means all comments will be collected\n",
    "    #startdate is how far back in time you want to retrieve comments, e.g., '11/14/2015'\n",
    "    #freqspec is the frequency interval used for searching; we will loop at this interval from current date all the way back to start date. E.g., 'D' for day.\n",
    "\n",
    "    r = praw.Reddit('Stimscrape 1.0 by stimscraper')\n",
    "    r.login('stimscraper', 'stimscraperpassword',disable_warning='True')\n",
    "    subreddit = r.get_subreddit(subreddit_string)\n",
    "\n",
    "    date_range = list(pd.date_range(start=startdate, end=enddate, freq=freqspec)) #populates date range with specified frequency\n",
    "\n",
    "    comment_dict={'authors':[],'comments':[],'links':[]} #where our comments will be stored\n",
    "\n",
    "    for lower_timestamp, upper_timestamp in zip(date_range, date_range[1:]):\n",
    "        print 'Starting new reddit search query for: ', lower_timestamp\n",
    "        # Convert timestamps from pd.Timestamp to epoch, offset by 1 second to avoid overlap\n",
    "        lower_timestamp_epoch = int(lower_timestamp.value / 1e9)\n",
    "        upper_timestamp_epoch = int((upper_timestamp.value / 1e9) - 1)\n",
    "\n",
    "        # Create query for timeframe\n",
    "        query = 'timestamp:%d..%d' % (lower_timestamp_epoch, upper_timestamp_epoch)\n",
    "        submissions = r.search(query, subreddit=subreddit_string, sort='new', limit=count, syntax='cloudsearch')\n",
    "    \n",
    "         # Loop through the submissions\n",
    "        for submission in submissions:\n",
    "            print 'Starting on new submission, as of: ', datetime.datetime.now()\n",
    "            submission.replace_more_comments(limit=None, threshold=1) #this may take a while...\n",
    "            flat_comments = praw.helpers.flatten_tree(submission.comments) #this gives us more than just the top comment\n",
    "            for x in flat_comments:\n",
    "                print 'FOUND A COMMENT'\n",
    "                x=vars(x)\n",
    "                #print x\n",
    "                author=str(x['author'])\n",
    "                body=str(x['body'].encode('utf8'))\n",
    "                link=str(x['link_id'].encode('utf8'))\n",
    "                comment_dict['authors'].append(author)\n",
    "                comment_dict['comments'].append(tokenize_comment(body))\n",
    "                comment_dict['links'].append(link)\n",
    "    \n",
    "    return comment_dict\n",
    "\n",
    "comment_dict=collect_comments('opiates',None,'11/14/2010','11/16/2015','D')\n",
    "#print comment_dict\n",
    "\n",
    "df = pd.DataFrame.from_dict(comment_dict)\n",
    "#print df\n",
    "df.to_csv('Documents/Stimscrape/opiates_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
